# [Self-Supervised GAN to counter forgetting](https://marcpickett.com/cl2018/CL-2018_paper_23.pdf)

* [Longer version](https://arxiv.org/pdf/1811.11212.pdf). 
	In this paper, main argument is based on the fact in GANs, the discriminator training involves training a classifier with non-stationary label distribution. (The generator changes, therefore the label distribution changes also) The authors name this problem as *Discriminator* forgetting. While I agree that this is a non-stationary set-up in terms of labels, I am not sure how this is a scenario where we need to avoid catastrophic forgetting. (We actually want to forget what the happened earlier with the generator, right? - at least remembering everthing about the earlier states of the generator seems wrong  - This being said, I see that the generator explore the space, and we probably would not want to forget the areas visited - so some notion of memory seems to be good idea)
	Authors state that there exists multiple papers which takes into account the sequential aspect of discriminator training. Then, they cite few papers on this ([2, 15, 17, 19]). What is proposed in the paper to combat the non-stationarity is to artificially rotate the genearted and real images, and estimate the rotation angle with an extra head coming out of the discriminator. The results propose that what they suggest does something substantial. But I am not sure how much continual learning there is in the proposed solution.  
	All this being said, I enjoyed reading this paper because it made me look at GAN training as an online / continual learning problem. (Although the solution proposed in the paper is not really continual learning, or is it? ) 
